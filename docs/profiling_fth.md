# How to add a new model to the simulator?å¦‚ä½•å‘æ¨¡æ‹Ÿå™¨æ·»åŠ æ–°æ¨¡å‹ï¼Ÿ

## Structure of Profiling data æ€§èƒ½åˆ†ææ•°æ®çš„ç»“æ„

The profiling data is stored in the `data/profiling` directory. The profiling data is stored in CSV format. The profiling data is stored in the following format:
æ€§èƒ½åˆ†ææ•°æ®å­˜å‚¨åœ¨ data/profiling ç›®å½•ä¸­ã€‚æ€§èƒ½åˆ†ææ•°æ®ä»¥ CSV æ ¼å¼å­˜å‚¨ã€‚æ€§èƒ½åˆ†ææ•°æ®çš„å­˜å‚¨æ ¼å¼å¦‚ä¸‹ï¼š
```yaml
    profiling/
        - compute
            - a100
                - codellama/CodeLlama-34b-Instruct-hf/
                    - mlp.csv
                    - attention.csv
                - internlm/internlm-20b/
                    - mlp.csv
                    - attention.csv
            - h100
                - meta-llama/Llama-2-70b-hf/
                    - mlp.csv
                    - attention.csv
        - network
            - a100_pair_nvlink
                - allreduce.csv
                - send_recv.csv
            - h100_dgx
                - allreduce.csv
                - send_recv.csv
```

For compute profiling, only the SKU matters not the network configuration of the node. So, we don't discriminate between `a100_pair_nvlink` (Azure Standard_NC96ads_A100_v4 with 4 A100s) and `a100_dgx` (A100 DGX with 8 A100s), the same compute data is used in both in the folder called `a100`.
å¯¹äºè®¡ç®—æ€§èƒ½åˆ†æï¼Œé‡è¦çš„æ˜¯ SKU è€Œä¸æ˜¯èŠ‚ç‚¹çš„ç½‘ç»œé…ç½®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸ä¼šåŒºåˆ† a100_pair_nvlinkï¼ˆAzure Standard_NC96ads_A100_v4ï¼Œå¸¦ 4 ä¸ª A100ï¼‰å’Œ a100_dgxï¼ˆA100 DGXï¼Œå¸¦ 8 ä¸ª A100ï¼‰ï¼Œåœ¨åä¸º a100 çš„æ–‡ä»¶å¤¹ä¸­ä½¿ç”¨ç›¸åŒçš„è®¡ç®—æ•°æ®ã€‚

For network profiling, the network configuration of the node matters. So, we have separate folders for `a100_pair_nvlink` and `a100_dgx`. One example is that TP4 is different in these. In `a100_pair_nvlink`, there are two pairs connected by NVLink but between these pairs is a relatively slower link, but in `a100_dgx` where all 8 GPUs are connected by NVLink.
å¯¹äºç½‘ç»œæ€§èƒ½åˆ†æï¼ŒèŠ‚ç‚¹çš„ç½‘ç»œé…ç½®å¾ˆé‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸º a100_pair_nvlink å’Œ a100_dgx è®¾ç½®äº†ä¸åŒçš„æ–‡ä»¶å¤¹ã€‚ä¸€ä¸ªä¾‹å­æ˜¯ï¼ŒTP4 åœ¨è¿™äº›é…ç½®ä¸­æ˜¯ä¸åŒçš„ã€‚åœ¨ a100_pair_nvlink ä¸­ï¼Œæœ‰ä¸¤ä¸ªé€šè¿‡ NVLink è¿æ¥çš„å¯¹ï¼Œä½†è¿™äº›å¯¹ä¹‹é—´çš„è¿æ¥ç›¸å¯¹è¾ƒæ…¢ï¼Œè€Œåœ¨ a100_dgx ä¸­ï¼Œæ‰€æœ‰ 8 ä¸ª GPU éƒ½é€šè¿‡ NVLink è¿æ¥ã€‚


## Adding a new model æ·»åŠ æ–°æ¨¡å‹

We need actual GPUs to get profiling data for a new model. Once the profiling is done, simulations can be run on CPUs only.
æˆ‘ä»¬éœ€è¦å®é™…çš„ GPU æ¥è·å–æ–°æ¨¡å‹çš„æ€§èƒ½åˆ†ææ•°æ®ã€‚ä¸€æ—¦æ€§èƒ½åˆ†æå®Œæˆï¼Œæ¨¡æ‹Ÿå¯ä»¥åœ¨ä»…ä½¿ç”¨ CPU çš„æƒ…å†µä¸‹è¿è¡Œã€‚

1. Clone the [`sarathi-serve`](https://github.com/microsoft/sarathi-serve) GitHub repo. å…‹éš† [`sarathi-serve`]
        git clone git@github.com:tianhao909/sarathi-serve.git
        fth git clone
        git branch 
        git fetch origin
        git branch -r
        git checkout -b vidur origin/vidur
        conda activate /app/software1/sarathi-serve_viudr/sarathi-serve/env
    a. Checkout branch [`vidur`](https://github.com/microsoft/sarathi-serve/tree/vidur) æ£€å‡ºåˆ†æ”¯ [`vidur`]
    b. Follow its README to install it. æŒ‰ç…§å…¶ README å®‰è£…ã€‚
    c. Let us assume that the Python virtual environment was created in `sarathi-serve/env`. å‡è®¾ Python è™šæ‹Ÿç¯å¢ƒå·²åœ¨ `sarathi-serve/env` ä¸­åˆ›å»ºã€‚

2. Now clone this repo [`vidur`](https://github.com/microsoft/vidur) but keep the `sarathi-serve/env` virtual environment activated. ç°åœ¨å…‹éš†æ­¤ä»“åº“ [`vidur`](https://github.com/microsoft/vidur)ï¼Œä½†ä¿æŒ `sarathi-serve/env` è™šæ‹Ÿç¯å¢ƒæ¿€æ´»ã€‚
        conda activate /app/software1/sarathi-serve_viudr/sarathi-serve/env
        
3. Add a YAML model config for the new model in `data/model_configs`.  
    åœ¨ `data/model_configs` ä¸­ä¸ºæ–°æ¨¡å‹æ·»åŠ ä¸€ä¸ª YAML æ¨¡å‹é…ç½®ã€‚
    - Use the model's HuggingFace model id for the file name eg. `data/model_configs/meta-llama/Llama-2-70b-hf.yml`. 
        ä½¿ç”¨æ¨¡å‹çš„ HuggingFace æ¨¡å‹ ID ä½œä¸ºæ–‡ä»¶åï¼Œä¾‹å¦‚ `data/model_configs/meta-llama/Llama-2-70b-hf.yml`ã€‚
        fth 
        https://huggingface.co/meta-llama/Llama-2-70b-hf
        https://hf-mirror.com/meta-llama/Llama-2-70b-hf


    - Refer HuggingFace `config.json` for the model eg. <https://huggingface.co/meta-llama/Llama-2-70b-hf/blob/main/config.json>. 
        å‚è€ƒ HuggingFace `config.json` æ¥é…ç½®æ¨¡å‹ï¼Œä¾‹å¦‚ <https://huggingface.co/meta-llama/Llama-2-70b-hf/blob/main/config.json>ã€‚

    - Ensure that correct parameters are set in the YAML file so that the reference transformer model [GPTModel](vidur/profiling/mlp/mlp_impl.py) closely resembles the new model.
        ç¡®ä¿åœ¨ YAML æ–‡ä»¶ä¸­è®¾ç½®äº†æ­£ç¡®çš„å‚æ•°ï¼Œä»¥ä¾¿å‚è€ƒçš„ transformer æ¨¡å‹ [GPTModel](vidur/profiling/mlp/mlp_impl.py) ä¸æ–°æ¨¡å‹éå¸¸ç›¸ä¼¼ã€‚
    - We use this reference model to profile only the MLP operations of all the models so the attention operations are no-op'ed here.
        æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªå‚è€ƒæ¨¡å‹ä»…åˆ†ææ‰€æœ‰æ¨¡å‹çš„ MLP æ“ä½œï¼Œå› æ­¤åœ¨è¿™é‡Œæ³¨æ„åŠ›æ“ä½œæ˜¯æ— æ“ä½œçš„ã€‚

4. Run the following command to install the simulator in the virtual environment: `python -m pip install -e .` from the `vidur/` directory.
    è¿è¡Œä»¥ä¸‹å‘½ä»¤åœ¨è™šæ‹Ÿç¯å¢ƒä¸­å®‰è£…æ¨¡æ‹Ÿå™¨ï¼š`python -m pip install -e .` ä» `vidur/` ç›®å½•ã€‚

5. For compute profiling (mlp and attention), 1 GPU is enough even for tensor parallel degrees greater than 1. So `num_gpus` set to 1 is sufficient albeit slower for profiling.
    å¯¹äºè®¡ç®—æ€§èƒ½åˆ†æï¼ˆmlp å’Œ attentionï¼‰ï¼Œå³ä½¿æ˜¯å¯¹äºå¤§äº 1 çš„å¼ é‡å¹¶è¡Œåº¦ï¼Œ1 ä¸ª GPU å°±è¶³å¤Ÿäº†ã€‚å› æ­¤ï¼Œè®¾ç½® `num_gpus` ä¸º 1 æ˜¯è¶³å¤Ÿçš„ï¼Œå°½ç®¡è¿™æ ·åœ¨æ€§èƒ½åˆ†ææ—¶ä¼šæ…¢ä¸€äº›ã€‚
    
6. Now we need to do the MLP profiling: ç°åœ¨æˆ‘ä»¬éœ€è¦è¿›è¡Œ MLP æ€§èƒ½åˆ†æï¼š

    ```bash
        python vidur/profiling/mlp/main.py \
        --models codellama/CodeLlama-34b-Instruct-hf \
        --num_gpus 4

        fth

         python -m vidur.profiling.mlp.main \
        --models codellama/CodeLlama-34b-Instruct-hf \
        --num_gpus 4

        python -m vidur.profiling.mlp.main \
        --models codellama/CodeLlama-34b-Instruct-hf \
        --num_gpus 1

        python -m vidur.profiling.mlp.main \
        --models meta-llama/Llama-2-70b-hf \
        --num_gpus 1
    ```

    - Run `python vidur/profiling/mlp/main.py --help` for more options.
        è¿è¡Œ `python vidur/profiling/mlp/main.py --help` ä»¥è·å–æ›´å¤šé€‰é¡¹ã€‚
    - Copy the CSV file from `profiling_outputs/mlp/<timestamp>/codellama/CodeLlama-34b-Instruct-hf/mlp.csv` to `data/profiling/compute/a100/codellama/CodeLlama-34b-Instruct-hf/mlp.csv`.
        å°† CSV æ–‡ä»¶ä» `profiling_outputs/mlp/<timestamp>/codellama/CodeLlama-34b-Instruct-hf/mlp.csv` å¤åˆ¶åˆ° `data/profiling/compute/a100/codellama/CodeLlama-34b-Instruct-hf/mlp.csv`ã€‚
1. Now we need to do the attention profiling:
     ç°åœ¨æˆ‘ä»¬éœ€è¦è¿›è¡Œæ³¨æ„åŠ›æ€§èƒ½åˆ†æï¼š
        pip install pandas -i https://mirrors.aliyun.com/pypi/simple/
        pip install ray -i https://mirrors.aliyun.com/pypi/simple/
        pip install -r require.txt -i https://mirrors.aliyun.com/pypi/simple/
        pip install tqdm -i https://mirrors.aliyun.com/pypi/simple/
        pip install sarathi -i https://mirrors.aliyun.com/pypi/simple/


        å®‰è£…pytorch
        https://pytorch.org/

        https://pytorch.org/get-started/previous-versions/
        conda install pytorch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 pytorch-cuda=12.1 -c pytorch -c nvidia

    ```bash
        python vidur/profiling/attention/main.py \
        --models codellama/CodeLlama-34b-Instruct-hf \
        --num_gpus 4
    ```

    - Run `python vidur/profiling/attention/main.py --help` for more options.
        è¿è¡Œ `python vidur/profiling/attention/main.py --help` ä»¥è·å–æ›´å¤šé€‰é¡¹ã€‚
    - Copy the CSV file from `profiling_outputs/attention/<timestamp>/codellama/CodeLlama-34b-Instruct-hf/attention.csv` to `data/profiling/compute/a100/codellama/CodeLlama-34b-Instruct-hf/attention.csv`.
        å°† CSV æ–‡ä»¶ä» `profiling_outputs/attention/<timestamp>/codellama/CodeLlama-34b-Instruct-hf/attention.csv` å¤åˆ¶åˆ° `data/profiling/compute/a100/codellama/CodeLlama-34b-Instruct-hf/attention.csv`ã€‚
    - Note that we are using `a100` as the device name. If you are using `h100` or some other device, then you need to create a new folder for that device in `data/profiling/compute` and copy the CSV files there.
        æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨ `a100` ä½œä¸ºè®¾å¤‡åç§°ã€‚å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ `h100` æˆ–å…¶ä»–è®¾å¤‡ï¼Œåˆ™éœ€è¦åœ¨ `data/profiling/compute` ä¸­ä¸ºè¯¥è®¾å¤‡åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶å¤¹ï¼Œå¹¶å°† CSV æ–‡ä»¶å¤åˆ¶åˆ°é‚£é‡Œã€‚



## Network (Collectives) profiling ç½‘ç»œï¼ˆé›†ä½“æ“ä½œï¼‰æ€§èƒ½åˆ†æ

Network profiling is not dependent on the model ğŸ‰. So, we can use the same network profiling data for all models. However, we need to ensure that the network profiling data is available for the node configuration we are using. If not, then we need to profile the network for the device. 1.
ç½‘ç»œæ€§èƒ½åˆ†æä¸ä¾èµ–äºæ¨¡å‹ğŸ‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç›¸åŒçš„ç½‘ç»œæ€§èƒ½åˆ†ææ•°æ®æ¥åˆ†ææ‰€æœ‰æ¨¡å‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿æˆ‘ä»¬ä½¿ç”¨çš„èŠ‚ç‚¹é…ç½®æœ‰å¯ç”¨çš„ç½‘ç»œæ€§èƒ½åˆ†ææ•°æ®ã€‚å¦‚æœæ²¡æœ‰ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦ä¸ºè®¾å¤‡è¿›è¡Œç½‘ç»œæ€§èƒ½åˆ†æã€‚

For network profiling, the node setup i.e. type of connectivity between the gpus matter. This is why we have the concept of `network_device`. The network_device is an informal name for the network configuration of the node. Eg: `a100_pair_nvlink`, `a100_dgx`, `h100_dgx` etc.
å¯¹äºç½‘ç»œæ€§èƒ½åˆ†æï¼ŒèŠ‚ç‚¹è®¾ç½®å³ GPU ä¹‹é—´çš„è¿æ¥ç±»å‹å¾ˆé‡è¦ã€‚è¿™å°±æ˜¯æˆ‘ä»¬æœ‰ `network_device` æ¦‚å¿µçš„åŸå› ã€‚`network_device` æ˜¯èŠ‚ç‚¹ç½‘ç»œé…ç½®çš„éæ­£å¼åç§°ã€‚ä¾‹å¦‚ï¼š`a100_pair_nvlink`ã€`a100_dgx`ã€`h100_dgx` ç­‰ã€‚

    1. For tensor parallelism, 4 GPUs are needed for TP4 and 8 GPUs are needed for TP8 etc.
        å¯¹äºå¼ é‡å¹¶è¡Œï¼ŒTP4 éœ€è¦ 4 ä¸ª GPUï¼ŒTP8 éœ€è¦ 8 ä¸ª GPU ç­‰ã€‚
    2. For pipeline parallelism across nodes, 2 nodes are needed to profile the link between the nodes.
        å¯¹äºè·¨èŠ‚ç‚¹çš„æµæ°´çº¿å¹¶è¡Œï¼Œéœ€è¦ 2 ä¸ªèŠ‚ç‚¹æ¥åˆ†æèŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥ã€‚

Currently available data include: ç›®å‰å¯ç”¨çš„æ•°æ®åŒ…æ‹¬ï¼š

- `a100_pair_nvlink`: Azure Standard_NC96ads_A100_v4 with 4 80GB A100 PCIe GPUs with pair-wise NVLINK connectivity.
- `h100_pair_nvlink`: Azure internal VM with 4 80GB H100 NVL GPUs with pair-wise NVLINK connectivity.
- `a100_dgx`: A100 DGX with 8 80GB A100s.
- `h100_dgx`: H100 DGX with 8 H100s.


- `a100_pair_nvlink`ï¼šAzure Standard_NC96ads_A100_v4ï¼Œå¸¦æœ‰ 4 ä¸ª 80GB A100 PCIe GPUï¼Œå…·æœ‰æˆå¯¹ NVLINK è¿æ¥ã€‚
- `h100_pair_nvlink`ï¼šAzure å†…éƒ¨ VMï¼Œå¸¦æœ‰ 4 ä¸ª 80GB H100 NVL GPUï¼Œå…·æœ‰æˆå¯¹ NVLINK è¿æ¥ã€‚
- `a100_dgx`ï¼šA100 DGXï¼Œå¸¦æœ‰ 8 ä¸ª 80GB A100ã€‚
- `h100_dgx`ï¼šH100 DGXï¼Œå¸¦æœ‰ 8 ä¸ª H100ã€‚


### Steps to profile: æ€§èƒ½åˆ†ææ­¥éª¤ï¼š

1. Clone this (`vidur`) repo and create a Python virtual environment as in [Setup](README.md).
    å…‹éš†æ­¤ï¼ˆ`vidur`ï¼‰ä»“åº“ï¼Œå¹¶æŒ‰ç…§[è®¾ç½®](README.md)ä¸­çš„æ–¹æ³•åˆ›å»º Python è™šæ‹Ÿç¯å¢ƒã€‚
2. Setup a ray cluster: è®¾ç½®ä¸€ä¸ª ray é›†ç¾¤ï¼š
    a. Tensor parallelism is typically done on a single node so we don't need a multi-node cluster.
        å¼ é‡å¹¶è¡Œé€šå¸¸åœ¨å•ä¸ªèŠ‚ç‚¹ä¸Šå®Œæˆï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦å¤šèŠ‚ç‚¹é›†ç¾¤ã€‚
    b. However, pipeline parallelism is typically done across multiple nodes so we need at least 2 nodes there.
        ç„¶è€Œï¼Œæµæ°´çº¿å¹¶è¡Œé€šå¸¸è·¨è¶Šå¤šä¸ªèŠ‚ç‚¹å®Œæˆï¼Œæ‰€ä»¥æˆ‘ä»¬è‡³å°‘éœ€è¦ 2 ä¸ªèŠ‚ç‚¹ã€‚
    c. Run `ray start --head` from the root node. ä»æ ¹èŠ‚ç‚¹è¿è¡Œ `ray start --head`ã€‚
    d. Run `ray start --address <head-node-ip>:<head-node-port>` from the other nodes. The other nodes also need to have the same git commit checked out.
        ä»å…¶ä»–èŠ‚ç‚¹è¿è¡Œ `ray start --address <head-node-ip>:<head-node-port>`ã€‚å…¶ä»–èŠ‚ç‚¹ä¹Ÿéœ€è¦æ£€å‡ºç›¸åŒçš„ git æäº¤ã€‚
3. Run the following command to profile for the `all_reduce` operation, (sufficient for TP):
    è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥åˆ†æ `all_reduce` æ“ä½œï¼ˆå¯¹äº TP è¶³å¤Ÿï¼‰ï¼š

    ```bash
        python vidur/profiling/collectives/main.py \
        --num_workers_per_node_combinations 1,2,4,8 \
        --collective all_reduce
    ```

    ```bash
        python vidur/profiling/collectives/main.py \
        --num_workers_per_node_combinations 2 \
        --collective all_reduce
    ```
    - One may need to adjust `--num_workers_per_node_combinations` depending on the number of GPUs in the node eg. `--num_workers_per_node_combinations 1,2,4` for Azure Standard_NC96ads_A100_v4 node.
        æ ¹æ®èŠ‚ç‚¹ä¸­çš„ GPU æ•°é‡ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ `--num_workers_per_node_combinations`ï¼Œä¾‹å¦‚å¯¹äº Azure Standard_NC96ads_A100_v4 èŠ‚ç‚¹ä½¿ç”¨ `--num_workers_per_node_combinations 1,2,4`ã€‚
    - Copy the CSV file from `profiling_outputs/collectives/<timestamp>/all_reduce.csv` to `data/profiling/network/{network_device}/allreduce.csv`.
        å°† CSV æ–‡ä»¶ä» `profiling_outputs/collectives/<timestamp>/all_reduce.csv` å¤åˆ¶åˆ° `data/profiling/network/{network_device}/allreduce.csv`ã€‚
    - `network_device` is an informal name for the network configuration of the node. Eg: `a100_pair_nvlink`, `a100_dgx`, `h100_dgx` etc.
        `network_device` æ˜¯èŠ‚ç‚¹ç½‘ç»œé…ç½®çš„éæ­£å¼åç§°ã€‚ä¾‹å¦‚ï¼š`a100_pair_nvlink`ã€`a100_dgx`ã€`h100_dgx` ç­‰ã€‚
    - Run `python vidur/profiling/collectives/main.py --help` for more options.
        è¿è¡Œ `python vidur/profiling/collectives/main.py --help` ä»¥è·å–æ›´å¤šé€‰é¡¹ã€‚



1. Run the following command to profile for the `send_recv` operation, (required only for PP):
    è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥åˆ†æ `send_recv` æ“ä½œï¼ˆä»… PP éœ€è¦ï¼‰ï¼š

    ```bash
        python vidur/profiling/collectives/main.py \
        --num_workers_per_node_combinations 1,2,4,8 \
        --collective send_recv
    ```

    ```bash
    fth
        python /app/software1/vidur/vidur/profiling/collectives/main.py \
        --num_workers_per_node_combinations 1,2,4,8 \
        --collective send_recv

        python /app/software1/vidur/vidur/profiling/collectives/main.py \
        --num_workers_per_node_combinations 2 \
        --collective send_recv

        python /app/software1/vidur/vidur/profiling/collectives/main.py \
        --num_workers_per_node_combinations 1 \
        --collective send_recv
    ```

    - Typically, PP is done across nodes so `num_workers_per_node_combinations` should be the same as the number of GPUs available in one node. Profiling `num_workers_per_node_combinations` less than the number of GPUs in the node to have PP inside a node. This can be useful when each gpu is not connected to every other gpu using the same high speed link.
        é€šå¸¸ï¼ŒPP æ˜¯è·¨èŠ‚ç‚¹å®Œæˆçš„ï¼Œæ‰€ä»¥ `num_workers_per_node_combinations` åº”è¯¥ä¸å•ä¸ªèŠ‚ç‚¹ä¸­å¯ç”¨çš„ GPU æ•°é‡ç›¸åŒã€‚åˆ†æ `num_workers_per_node_combinations` å°äºèŠ‚ç‚¹ä¸­ GPU æ•°é‡çš„ PP åœ¨èŠ‚ç‚¹å†…éƒ¨ã€‚å½“æ¯ä¸ª GPU ä¸æ˜¯é€šè¿‡ç›¸åŒçš„é«˜é€Ÿé“¾æ¥è¿æ¥åˆ°æ¯ä¸ªå…¶ä»– GPU æ—¶ï¼Œè¿™å¯èƒ½ä¼šå¾ˆæœ‰ç”¨ã€‚
    - Copy the CSV file from `profiling_outputs/collectives/<timestamp>/send_recv.csv` to `data/profiling/network/{network_device}/send_recv.csv`.
        å°† CSV æ–‡ä»¶ä» `profiling_outputs/collectives/<timestamp>/send_recv.csv` å¤åˆ¶åˆ° `data/profiling/network/{network_device}/send_recv.csv`ã€‚
    - `network_device` is an informal name for the network configuration of the node. Eg: `a100_pair_nvlink`, `a100_dgx`, `h100_dgx` etc.
        `network_device` æ˜¯èŠ‚ç‚¹ç½‘ç»œé…ç½®çš„éæ­£å¼åç§°ã€‚ä¾‹å¦‚ï¼š`a100_pair_nvlink`ã€`a100_dgx`ã€`h100_dgx` ç­‰ã€‚



## CPU Overhead Profiling  CPU å¼€é”€æ€§èƒ½åˆ†æ

These include implementation overheads like scheduling time, sampling time, detokenization etc. For better fidelity, these should also be profiled. However, they tie the simulator closely to the implementation eg. `vLLM`. Scripts are available [here](vidur/profiling/cpu_overhead/) but not documented yet. These scripts follow a similar pattern to the compute and network profiling scripts.
è¿™äº›åŒ…æ‹¬å®ç°å¼€é”€ï¼Œå¦‚è°ƒåº¦æ—¶é—´ã€é‡‡æ ·æ—¶é—´ã€å»æ ‡è®°åŒ–ç­‰ã€‚ä¸ºäº†æ›´å¥½çš„ä¿çœŸåº¦ï¼Œè¿™äº›ä¹Ÿåº”è¯¥è¿›è¡Œæ€§èƒ½åˆ†æã€‚ç„¶è€Œï¼Œå®ƒä»¬å°†æ¨¡æ‹Ÿå™¨ä¸å®ç°ç´§å¯†ç»‘å®šï¼Œä¾‹å¦‚ `vLLM`ã€‚è„šæœ¬å¯åœ¨[æ­¤å¤„](vidur/profiling/cpu_overhead/)æ‰¾åˆ°ï¼Œä½†å°šæœªæ–‡æ¡£åŒ–ã€‚è¿™äº›è„šæœ¬éµå¾ªä¸è®¡ç®—å’Œç½‘ç»œæ€§èƒ½åˆ†æè„šæœ¬ç±»ä¼¼çš„æ¨¡å¼ã€‚





