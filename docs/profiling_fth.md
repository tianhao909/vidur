# How to add a new model to the simulator?å¦‚ä½•å‘æ¨¡æ‹Ÿå™¨æ·»åŠ æ–°æ¨¡å‹ï¼Ÿ

## Structure of Profiling data æ€§èƒ½åˆ†ææ•°æ®çš„ç»“æ„

The profiling data is stored in the `data/profiling` directory. The profiling data is stored in CSV format. The profiling data is stored in the following format:


æ€§èƒ½åˆ†ææ•°æ®å­˜å‚¨åœ¨ data/profiling ç›®å½•ä¸­ã€‚æ€§èƒ½åˆ†ææ•°æ®ä»¥ CSV æ ¼å¼å­˜å‚¨ã€‚æ€§èƒ½åˆ†ææ•°æ®çš„å­˜å‚¨æ ¼å¼å¦‚ä¸‹ï¼š
```yaml
    profiling/
        - compute
            - a100
                - codellama/CodeLlama-34b-Instruct-hf/
                    - mlp.csv
                    - attention.csv
                - internlm/internlm-20b/
                    - mlp.csv
                    - attention.csv
            - h100
                - meta-llama/Llama-2-70b-hf/
                    - mlp.csv
                    - attention.csv
        - network
            - a100_pair_nvlink
                - allreduce.csv
                - send_recv.csv
            - h100_dgx
                - allreduce.csv
                - send_recv.csv
```

For compute profiling, only the SKU matters not the network configuration of the node. So, we don't discriminate between `a100_pair_nvlink` (Azure Standard_NC96ads_A100_v4 with 4 A100s) and `a100_dgx` (A100 DGX with 8 A100s), the same compute data is used in both in the folder called `a100`.
For network profiling, the network configuration of the node matters. So, we have separate folders for `a100_pair_nvlink` and `a100_dgx`. One example is that TP4 is different in these. In `a100_pair_nvlink`, there are two pairs connected by NVLink but between these pairs is a relatively slower link, but in `a100_dgx` where all 8 GPUs are connected by NVLink.

å¯¹äºè®¡ç®—æ€§èƒ½åˆ†æï¼Œé‡è¦çš„æ˜¯ SKU è€Œä¸æ˜¯èŠ‚ç‚¹çš„ç½‘ç»œé…ç½®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸ä¼šåŒºåˆ† a100_pair_nvlinkï¼ˆAzure Standard_NC96ads_A100_v4ï¼Œå¸¦ 4 ä¸ª A100ï¼‰å’Œ a100_dgxï¼ˆA100 DGXï¼Œå¸¦ 8 ä¸ª A100ï¼‰ï¼Œåœ¨åä¸º a100 çš„æ–‡ä»¶å¤¹ä¸­ä½¿ç”¨ç›¸åŒçš„è®¡ç®—æ•°æ®ã€‚
å¯¹äºç½‘ç»œæ€§èƒ½åˆ†æï¼ŒèŠ‚ç‚¹çš„ç½‘ç»œé…ç½®å¾ˆé‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸º a100_pair_nvlink å’Œ a100_dgx è®¾ç½®äº†ä¸åŒçš„æ–‡ä»¶å¤¹ã€‚ä¸€ä¸ªä¾‹å­æ˜¯ï¼ŒTP4 åœ¨è¿™äº›é…ç½®ä¸­æ˜¯ä¸åŒçš„ã€‚åœ¨ a100_pair_nvlink ä¸­ï¼Œæœ‰ä¸¤ä¸ªé€šè¿‡ NVLink è¿æ¥çš„å¯¹ï¼Œä½†è¿™äº›å¯¹ä¹‹é—´çš„è¿æ¥ç›¸å¯¹è¾ƒæ…¢ï¼Œè€Œåœ¨ a100_dgx ä¸­ï¼Œæ‰€æœ‰ 8 ä¸ª GPU éƒ½é€šè¿‡ NVLink è¿æ¥ã€‚


## Adding a new model æ·»åŠ æ–°æ¨¡å‹

We need actual GPUs to get profiling data for a new model. Once the profiling is done, simulations can be run on CPUs only.

æˆ‘ä»¬éœ€è¦å®é™…çš„ GPU æ¥è·å–æ–°æ¨¡å‹çš„æ€§èƒ½åˆ†ææ•°æ®ã€‚ä¸€æ—¦æ€§èƒ½åˆ†æå®Œæˆï¼Œæ¨¡æ‹Ÿå¯ä»¥åœ¨ä»…ä½¿ç”¨ CPU çš„æƒ…å†µä¸‹è¿è¡Œã€‚

1. Clone the [`sarathi-serve`](https://github.com/microsoft/sarathi-serve) GitHub repo.
    1. Checkout branch [`vidur`](https://github.com/microsoft/sarathi-serve/tree/vidur)
    1. Follow its README to install it.
    1. Let us assume that the Python virtual environment was created in `sarathi-serve/env`.
1. Now clone this repo [`vidur`](https://github.com/microsoft/vidur) but keep the `sarathi-serve/env` virtual environment activated.
1. Add a YAML model config for the new model in `data/model_configs`.
    - Use the model's HuggingFace model id for the file name eg. `data/model_configs/meta-llama/Llama-2-70b-hf.yml`.
    - Refer HuggingFace `config.json` for the model eg. <https://huggingface.co/meta-llama/Llama-2-70b-hf/blob/main/config.json>.
    - Ensure that correct parameters are set in the YAML file so that the reference transformer model [GPTModel](vidur/profiling/mlp/mlp_impl.py) closely resembles the new model.
    - We use this reference model to profile only the MLP operations of all the models so the attention operations are no-op'ed here.
1. Run the following command to install the simulator in the virtual environment: `python -m pip install -e .` from the `vidur/` directory.
1. For compute profiling (mlp and attention), 1 GPU is enough even for tensor parallel degrees greater than 1. So `num_gpus` set to 1 is sufficient albeit slower for profiling.
1. Now we need to do the MLP profiling:

1. å…‹éš† [`sarathi-serve`](https://github.com/microsoft/sarathi-serve) GitHub ä»“åº“ã€‚
    1. æ£€å‡ºåˆ†æ”¯ [`vidur`](https://github.com/microsoft/sarathi-serve/tree/vidur)ã€‚
    1. æŒ‰ç…§å…¶ README å®‰è£…ã€‚
    1. å‡è®¾ Python è™šæ‹Ÿç¯å¢ƒå·²åœ¨ `sarathi-serve/env` ä¸­åˆ›å»ºã€‚
1. ç°åœ¨å…‹éš†æ­¤ä»“åº“ [`vidur`](https://github.com/microsoft/vidur)ï¼Œä½†ä¿æŒ `sarathi-serve/env` è™šæ‹Ÿç¯å¢ƒæ¿€æ´»ã€‚
1. åœ¨ `data/model_configs` ä¸­ä¸ºæ–°æ¨¡å‹æ·»åŠ ä¸€ä¸ª YAML æ¨¡å‹é…ç½®ã€‚
    - ä½¿ç”¨æ¨¡å‹çš„ HuggingFace æ¨¡å‹ ID ä½œä¸ºæ–‡ä»¶åï¼Œä¾‹å¦‚ `data/model_configs/meta-llama/Llama-2-70b-hf.yml`ã€‚
    - å‚è€ƒ HuggingFace `config.json` æ¥é…ç½®æ¨¡å‹ï¼Œä¾‹å¦‚ <https://huggingface.co/meta-llama/Llama-2-70b-hf/blob/main/config.json>ã€‚
    - ç¡®ä¿åœ¨ YAML æ–‡ä»¶ä¸­è®¾ç½®äº†æ­£ç¡®çš„å‚æ•°ï¼Œä»¥ä¾¿å‚è€ƒçš„ transformer æ¨¡å‹ [GPTModel](vidur/profiling/mlp/mlp_impl.py) ä¸æ–°æ¨¡å‹éå¸¸ç›¸ä¼¼ã€‚
    - æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªå‚è€ƒæ¨¡å‹ä»…åˆ†ææ‰€æœ‰æ¨¡å‹çš„ MLP æ“ä½œï¼Œå› æ­¤åœ¨è¿™é‡Œæ³¨æ„åŠ›æ“ä½œæ˜¯æ— æ“ä½œçš„ã€‚
1. è¿è¡Œä»¥ä¸‹å‘½ä»¤åœ¨è™šæ‹Ÿç¯å¢ƒä¸­å®‰è£…æ¨¡æ‹Ÿå™¨ï¼š`python -m pip install -e .` ä» `vidur/` ç›®å½•ã€‚
1. å¯¹äºè®¡ç®—æ€§èƒ½åˆ†æï¼ˆmlp å’Œ attentionï¼‰ï¼Œå³ä½¿æ˜¯å¯¹äºå¤§äº 1 çš„å¼ é‡å¹¶è¡Œåº¦ï¼Œ1 ä¸ª GPU å°±è¶³å¤Ÿäº†ã€‚å› æ­¤ï¼Œè®¾ç½® `num_gpus` ä¸º 1 æ˜¯è¶³å¤Ÿçš„ï¼Œå°½ç®¡è¿™æ ·åœ¨æ€§èƒ½åˆ†ææ—¶ä¼šæ…¢ä¸€äº›ã€‚
1. ç°åœ¨æˆ‘ä»¬éœ€è¦è¿›è¡Œ MLP æ€§èƒ½åˆ†æï¼š

    ```bash
        python vidur/profiling/mlp/main.py \
        --models codellama/CodeLlama-34b-Instruct-hf \
        --num_gpus 4
    ```

    - Run `python vidur/profiling/mlp/main.py --help` for more options.
    - Copy the CSV file from `profiling_outputs/mlp/<timestamp>/codellama/CodeLlama-34b-Instruct-hf/mlp.csv` to `data/profiling/compute/a100/codellama/CodeLlama-34b-Instruct-hf/mlp.csv`.
1. Now we need to do the attention profiling:

    - è¿è¡Œ `python vidur/profiling/mlp/main.py --help` ä»¥è·å–æ›´å¤šé€‰é¡¹ã€‚
    - å°† CSV æ–‡ä»¶ä» `profiling_outputs/mlp/<timestamp>/codellama/CodeLlama-34b-Instruct-hf/mlp.csv` å¤åˆ¶åˆ° `data/profiling/compute/a100/codellama/CodeLlama-34b-Instruct-hf/mlp.csv`ã€‚
1. ç°åœ¨æˆ‘ä»¬éœ€è¦è¿›è¡Œæ³¨æ„åŠ›æ€§èƒ½åˆ†æï¼š

    ```bash
        python vidur/profiling/attention/main.py \
        --models codellama/CodeLlama-34b-Instruct-hf \
        --num_gpus 4
    ```

    - Run `python vidur/profiling/attention/main.py --help` for more options.
    - Copy the CSV file from `profiling_outputs/attention/<timestamp>/codellama/CodeLlama-34b-Instruct-hf/attention.csv` to `data/profiling/compute/a100/codellama/CodeLlama-34b-Instruct-hf/attention.csv`.
    - Note that we are using `a100` as the device name. If you are using `h100` or some other device, then you need to create a new folder for that device in `data/profiling/compute` and copy the CSV files there.

    - è¿è¡Œ `python vidur/profiling/attention/main.py --help` ä»¥è·å–æ›´å¤šé€‰é¡¹ã€‚
    - å°† CSV æ–‡ä»¶ä» `profiling_outputs/attention/<timestamp>/codellama/CodeLlama-34b-Instruct-hf/attention.csv` å¤åˆ¶åˆ° `data/profiling/compute/a100/codellama/CodeLlama-34b-Instruct-hf/attention.csv`ã€‚
    - æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨ `a100` ä½œä¸ºè®¾å¤‡åç§°ã€‚å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ `h100` æˆ–å…¶ä»–è®¾å¤‡ï¼Œåˆ™éœ€è¦åœ¨ `data/profiling/compute` ä¸­ä¸ºè¯¥è®¾å¤‡åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶å¤¹ï¼Œå¹¶å°† CSV æ–‡ä»¶å¤åˆ¶åˆ°é‚£é‡Œã€‚

ç”±äºç½‘ç»œåŸå› ï¼Œæˆ‘æ— æ³•æˆåŠŸè§£ææä¾›çš„é“¾æ¥ã€‚å¦‚æœæ‚¨éœ€è¦è¿™äº›ç½‘é¡µçš„å†…å®¹ï¼Œè¯·æ£€æŸ¥é“¾æ¥çš„åˆæ³•æ€§ï¼Œå¹¶åœ¨ç½‘ç»œç¨³å®šæ—¶é€‚å½“é‡è¯•ã€‚å¦‚æœæ‚¨æœ‰å…¶ä»–é—®é¢˜æˆ–éœ€è¦è¿›ä¸€æ­¥çš„å¸®åŠ©ï¼Œè¯·éšæ—¶å‘ŠçŸ¥ã€‚


## Network (Collectives) profiling

Network profiling is not dependent on the model ğŸ‰. So, we can use the same network profiling data for all models. However, we need to ensure that the network profiling data is available for the node configuration we are using. If not, then we need to profile the network for the device. 1.

For network profiling, the node setup i.e. type of connectivity between the gpus matter. This is why we have the concept of `network_device`. The network_device is an informal name for the network configuration of the node. Eg: `a100_pair_nvlink`, `a100_dgx`, `h100_dgx` etc.
    1. For tensor parallelism, 4 GPUs are needed for TP4 and 8 GPUs are needed for TP8 etc.
    2. For pipeline parallelism across nodes, 2 nodes are needed to profile the link between the nodes.

Currently available data include:

- `a100_pair_nvlink`: Azure Standard_NC96ads_A100_v4 with 4 80GB A100 PCIe GPUs with pair-wise NVLINK connectivity.
- `h100_pair_nvlink`: Azure internal VM with 4 80GB H100 NVL GPUs with pair-wise NVLINK connectivity.
- `a100_dgx`: A100 DGX with 8 80GB A100s.
- `h100_dgx`: H100 DGX with 8 H100s.

#ç½‘ç»œï¼ˆé›†ä½“æ“ä½œï¼‰æ€§èƒ½åˆ†æ

ç½‘ç»œæ€§èƒ½åˆ†æä¸ä¾èµ–äºæ¨¡å‹ğŸ‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç›¸åŒçš„ç½‘ç»œæ€§èƒ½åˆ†ææ•°æ®æ¥åˆ†ææ‰€æœ‰æ¨¡å‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿æˆ‘ä»¬ä½¿ç”¨çš„èŠ‚ç‚¹é…ç½®æœ‰å¯ç”¨çš„ç½‘ç»œæ€§èƒ½åˆ†ææ•°æ®ã€‚å¦‚æœæ²¡æœ‰ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦ä¸ºè®¾å¤‡è¿›è¡Œç½‘ç»œæ€§èƒ½åˆ†æã€‚

å¯¹äºç½‘ç»œæ€§èƒ½åˆ†æï¼ŒèŠ‚ç‚¹è®¾ç½®å³ GPU ä¹‹é—´çš„è¿æ¥ç±»å‹å¾ˆé‡è¦ã€‚è¿™å°±æ˜¯æˆ‘ä»¬æœ‰ `network_device` æ¦‚å¿µçš„åŸå› ã€‚`network_device` æ˜¯èŠ‚ç‚¹ç½‘ç»œé…ç½®çš„éæ­£å¼åç§°ã€‚ä¾‹å¦‚ï¼š`a100_pair_nvlink`ã€`a100_dgx`ã€`h100_dgx` ç­‰ã€‚

1. å¯¹äºå¼ é‡å¹¶è¡Œï¼ŒTP4 éœ€è¦ 4 ä¸ª GPUï¼ŒTP8 éœ€è¦ 8 ä¸ª GPU ç­‰ã€‚
2. å¯¹äºè·¨èŠ‚ç‚¹çš„æµæ°´çº¿å¹¶è¡Œï¼Œéœ€è¦ 2 ä¸ªèŠ‚ç‚¹æ¥åˆ†æèŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥ã€‚

ç›®å‰å¯ç”¨çš„æ•°æ®åŒ…æ‹¬ï¼š

- `a100_pair_nvlink`ï¼šAzure Standard_NC96ads_A100_v4ï¼Œå¸¦æœ‰ 4 ä¸ª 80GB A100 PCIe GPUï¼Œå…·æœ‰æˆå¯¹ NVLINK è¿æ¥ã€‚
- `h100_pair_nvlink`ï¼šAzure å†…éƒ¨ VMï¼Œå¸¦æœ‰ 4 ä¸ª 80GB H100 NVL GPUï¼Œå…·æœ‰æˆå¯¹ NVLINK è¿æ¥ã€‚
- `a100_dgx`ï¼šA100 DGXï¼Œå¸¦æœ‰ 8 ä¸ª 80GB A100ã€‚
- `h100_dgx`ï¼šH100 DGXï¼Œå¸¦æœ‰ 8 ä¸ª H100ã€‚


### Steps to profile:

1. Clone this (`vidur`) repo and create a Python virtual environment as in [Setup](README.md).
1. Setup a ray cluster:
    1. Tensor parallelism is typically done on a single node so we don't need a multi-node cluster.
    1. However, pipeline parallelism is typically done across multiple nodes so we need at least 2 nodes there.
    1. Run `ray start --head` from the root node.
    1. Run `ray start --address <head-node-ip>:<head-node-port>` from the other nodes. The other nodes also need to have the same git commit checked out.
1. Run the following command to profile for the `all_reduce` operation, (sufficient for TP):

    ```bash
        python vidur/profiling/collectives/main.py \
        --num_workers_per_node_combinations 1,2,4,8 \
        --collective all_reduce
    ```

    - One may need to adjust `--num_workers_per_node_combinations` depending on the number of GPUs in the node eg. `--num_workers_per_node_combinations 1,2,4` for Azure Standard_NC96ads_A100_v4 node.
    - Copy the CSV file from `profiling_outputs/collectives/<timestamp>/all_reduce.csv` to `data/profiling/network/{network_device}/allreduce.csv`.
    - `network_device` is an informal name for the network configuration of the node. Eg: `a100_pair_nvlink`, `a100_dgx`, `h100_dgx` etc.
    - Run `python vidur/profiling/collectives/main.py --help` for more options.
1. Run the following command to profile for the `send_recv` operation, (required only for PP):

    ```bash
        python vidur/profiling/collectives/main.py \
        --num_workers_per_node_combinations 1,2,4,8 \
        --collective send_recv
    ```

    - Typically, PP is done across nodes so `num_workers_per_node_combinations` should be the same as the number of GPUs available in one node. Profiling `num_workers_per_node_combinations` less than the number of GPUs in the node to have PP inside a node. This can be useful when each gpu is not connected to every other gpu using the same high speed link.
    - Copy the CSV file from `profiling_outputs/collectives/<timestamp>/send_recv.csv` to `data/profiling/network/{network_device}/send_recv.csv`.
    - `network_device` is an informal name for the network configuration of the node. Eg: `a100_pair_nvlink`, `a100_dgx`, `h100_dgx` etc.

æ€§èƒ½åˆ†ææ­¥éª¤ï¼š

1. å…‹éš†æ­¤ï¼ˆ`vidur`ï¼‰ä»“åº“ï¼Œå¹¶æŒ‰ç…§[è®¾ç½®](README.md)ä¸­çš„æ–¹æ³•åˆ›å»º Python è™šæ‹Ÿç¯å¢ƒã€‚
2. è®¾ç½®ä¸€ä¸ª ray é›†ç¾¤ï¼š
    1. å¼ é‡å¹¶è¡Œé€šå¸¸åœ¨å•ä¸ªèŠ‚ç‚¹ä¸Šå®Œæˆï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦å¤šèŠ‚ç‚¹é›†ç¾¤ã€‚
    2. ç„¶è€Œï¼Œæµæ°´çº¿å¹¶è¡Œé€šå¸¸è·¨è¶Šå¤šä¸ªèŠ‚ç‚¹å®Œæˆï¼Œæ‰€ä»¥æˆ‘ä»¬è‡³å°‘éœ€è¦ 2 ä¸ªèŠ‚ç‚¹ã€‚
    3. ä»æ ¹èŠ‚ç‚¹è¿è¡Œ `ray start --head`ã€‚
    4. ä»å…¶ä»–èŠ‚ç‚¹è¿è¡Œ `ray start --address <head-node-ip>:<head-node-port>`ã€‚å…¶ä»–èŠ‚ç‚¹ä¹Ÿéœ€è¦æ£€å‡ºç›¸åŒçš„ git æäº¤ã€‚
3. è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥åˆ†æ `all_reduce` æ“ä½œï¼ˆå¯¹äº TP è¶³å¤Ÿï¼‰ï¼š

    ```bash
        python vidur/profiling/collectives/main.py \
        --num_workers_per_node_combinations 1,2,4,8 \
        --collective all_reduce
    ```

    - æ ¹æ®èŠ‚ç‚¹ä¸­çš„ GPU æ•°é‡ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ `--num_workers_per_node_combinations`ï¼Œä¾‹å¦‚å¯¹äº Azure Standard_NC96ads_A100_v4 èŠ‚ç‚¹ä½¿ç”¨ `--num_workers_per_node_combinations 1,2,4`ã€‚
    - å°† CSV æ–‡ä»¶ä» `profiling_outputs/collectives/<timestamp>/all_reduce.csv` å¤åˆ¶åˆ° `data/profiling/network/{network_device}/allreduce.csv`ã€‚
    - `network_device` æ˜¯èŠ‚ç‚¹ç½‘ç»œé…ç½®çš„éæ­£å¼åç§°ã€‚ä¾‹å¦‚ï¼š`a100_pair_nvlink`ã€`a100_dgx`ã€`h100_dgx` ç­‰ã€‚
    - è¿è¡Œ `python vidur/profiling/collectives/main.py --help` ä»¥è·å–æ›´å¤šé€‰é¡¹ã€‚
4. è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥åˆ†æ `send_recv` æ“ä½œï¼ˆä»… PP éœ€è¦ï¼‰ï¼š

    ```bash
        python vidur/profiling/collectives/main.py \
        --num_workers_per_node_combinations 1,2,4,8 \
        --collective send_recv
    ```

    - é€šå¸¸ï¼ŒPP æ˜¯è·¨èŠ‚ç‚¹å®Œæˆçš„ï¼Œæ‰€ä»¥ `num_workers_per_node_combinations` åº”è¯¥ä¸å•ä¸ªèŠ‚ç‚¹ä¸­å¯ç”¨çš„ GPU æ•°é‡ç›¸åŒã€‚åˆ†æ `num_workers_per_node_combinations` å°äºèŠ‚ç‚¹ä¸­ GPU æ•°é‡çš„ PP åœ¨èŠ‚ç‚¹å†…éƒ¨ã€‚å½“æ¯ä¸ª GPU ä¸æ˜¯é€šè¿‡ç›¸åŒçš„é«˜é€Ÿé“¾æ¥è¿æ¥åˆ°æ¯ä¸ªå…¶ä»– GPU æ—¶ï¼Œè¿™å¯èƒ½ä¼šå¾ˆæœ‰ç”¨ã€‚
    - å°† CSV æ–‡ä»¶ä» `profiling_outputs/collectives/<timestamp>/send_recv.csv` å¤åˆ¶åˆ° `data/profiling/network/{network_device}/send_recv.csv`ã€‚
    - `network_device` æ˜¯èŠ‚ç‚¹ç½‘ç»œé…ç½®çš„éæ­£å¼åç§°ã€‚ä¾‹å¦‚ï¼š`a100_pair_nvlink`ã€`a100_dgx`ã€`h100_dgx` ç­‰ã€‚

## CPU Overhead Profiling

These include implementation overheads like scheduling time, sampling time, detokenization etc. For better fidelity, these should also be profiled. However, they tie the simulator closely to the implementation eg. `vLLM`. Scripts are available [here](vidur/profiling/cpu_overhead/) but not documented yet. These scripts follow a similar pattern to the compute and network profiling scripts.
CPU å¼€é”€æ€§èƒ½åˆ†æ

è¿™äº›åŒ…æ‹¬å®ç°å¼€é”€ï¼Œå¦‚è°ƒåº¦æ—¶é—´ã€é‡‡æ ·æ—¶é—´ã€å»æ ‡è®°åŒ–ç­‰ã€‚ä¸ºäº†æ›´å¥½çš„ä¿çœŸåº¦ï¼Œè¿™äº›ä¹Ÿåº”è¯¥è¿›è¡Œæ€§èƒ½åˆ†æã€‚ç„¶è€Œï¼Œå®ƒä»¬å°†æ¨¡æ‹Ÿå™¨ä¸å®ç°ç´§å¯†ç»‘å®šï¼Œä¾‹å¦‚ `vLLM`ã€‚è„šæœ¬å¯åœ¨[æ­¤å¤„](vidur/profiling/cpu_overhead/)æ‰¾åˆ°ï¼Œä½†å°šæœªæ–‡æ¡£åŒ–ã€‚è¿™äº›è„šæœ¬éµå¾ªä¸è®¡ç®—å’Œç½‘ç»œæ€§èƒ½åˆ†æè„šæœ¬ç±»ä¼¼çš„æ¨¡å¼ã€‚





